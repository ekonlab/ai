{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are in possession of labelled data, we can take a\n",
    "step further and use those labels in a supervised learning\n",
    "task, where the labels become our targets. In this chapter we\n",
    "will discuss how classification algorithms are used and\n",
    "scored. In particular we will cover some important\n",
    "algorithms such as K Nearest Neighbours, Logistic\n",
    "Regression and the famous Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a task that involves arranging\n",
    "objects systematically into appropriate groups or categories\n",
    "depending on the characteristics that define such groupings.\n",
    "It is important to emphasise that the groups are pre-defined\n",
    "according to established criteria. In our case, the use of classification is to determine the category to which an\n",
    "unseen observation belongs, depending on the information\n",
    "of a training dataset with appropriate labels. Classification\n",
    "is therefore a supervised learning task. Whereas in clustering\n",
    "the aim is to determine the groups from the features in the\n",
    "dataset, classification uses the labelled groups to predict\n",
    "the best category for unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very convenient way to evaluate the accuracy of a\n",
    "classifier is the use of a table that summarises the\n",
    "performance of our algorithm against the data provided.\n",
    "Karl Pearson used the name contingency table. The machine learning community tends to call it a confusion\n",
    "matrix as it lets us determine if the classifier is confusing\n",
    "two classes by assigning observations of one class to the\n",
    "other. One advantage of a confusion matrix is that it can be\n",
    "extended to cases with more than two categories. In any case, the contingency table or confusion matrix is\n",
    "organised in such a way that its columns are related to the\n",
    "instances in a predicted category, whereas its rows refer to\n",
    "actual classes. A False Positive is a case where we have incorrectly made\n",
    "a prediction for a positive detection. From the table we can see that the troop has predicted 6 cases as aircraft,\n",
    "but they turned out to be flocks. Finally, a False Negative\n",
    "is a case where we have incorrectly made a prediction\n",
    "for a negative detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall or True Positive Rate (TPR) : It is also known as sensitivity\n",
    "or hit rate. It corresponds to the proportion of positive data points that are correctly classified as positive versus the total\n",
    "number of positive points. The true positive rate is also known as recall or sensitivity (TP/TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Negative Rate or Specificity (TNR): It is the counterpart\n",
    "of the True Positive Rate as it measures the proportion of negatives that have been correctly identified. The true negative rate is also\n",
    "known as specificity (TN/TN+FP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fallout or False Positive Rate (FPR): It corresponds to the\n",
    "proportion of negative data points that are mistakenly considered as positive, with respect to all negative data\n",
    "points. The false positive rate is also\n",
    "known as fallout.(1-TNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision or Postitive Predictive Value (PPV): It is the proportion\n",
    "of positive results that are true positive results. The precision is also known as\n",
    "positive predictive value.(TP/TP+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is given by the ratio of the points that\n",
    "have been correctly classified and the total number of data\n",
    "points.(TP+TN)/(TP+FP+TN+FN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 AUC and ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operator Characteristic or ROC is a\n",
    "quantitative analysis technique used in binary classification. It lets us construct a curve in terms of the true positive rate against the false positive rate. Unfortunately\n",
    "ROC curves are suitable for binary classification problems\n",
    "only. In a ROC curve the True Positive Rate is plotted as a function of the False Positive Rate for different cut-off\n",
    "points or thresholds for the classifier. Think of these\n",
    "thresholds as settings in the receiver used by the radar\n",
    "operators. If our classifier is able to distinguish the two classes without\n",
    "overlap, then the ROC would have a point at the 100% sensitivity and 0% fallout, i.e. the upper left corner of the curve. This means that the closer the ROC curve is to that corner, then the better the accuracy of the classifier. It\n",
    "is clear that we would prefer classifiers that are better than guessing, in other words those whose ROC curve lies above\n",
    "the diagonal. Also we would prefer those classifiers whose\n",
    "ROC curves are closer to the curve given by the perfect\n",
    "classifier. If you end up with a ROC curve that lies below\n",
    "the diagonal, your classifier is worse than guessing, and it should be immediately discarded. AUC is the Area Under the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Classification with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the KNN classifier, similarity is given by the distance\n",
    "between points. We classify new observations taking into account the class of the k nearest labelled data points. This means that we need a distance measure between points, and we can start with the well-known Euclidean distance we\n",
    "discussed in Section 3.8. As it was the case in k-means for clustering, the value of k in KNN is a parameter that is given as an input to the algorithm. For a new unseen observation, we measure the\n",
    "distance to the rest of the points in the dataset and pick the\n",
    "k nearest points. We then simply take the most common\n",
    "class among these to be the class of the new observation. In\n",
    "terms of steps we have the following: 1. Choose a value for k as an input. 2. Select the k nearest data points to the new observation. 3. Find the most common class among the k points chosen. 4. Assign this class to the new observation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\n",
    "XTrain,XTest,YTrain,YTest = ms.train_test_split(X,Y,test_size=0.3,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n       error_score='raise-deprecating',\n       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n           weights='uniform'),\n       fit_params=None, iid='warn', n_jobs=None,\n       param_grid=[{'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]}],\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Find the appropriate value of \"k\" using gridsearch\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# search between 1 and 20 and find best value of k using cross-validation\n",
    "k_neighbours = list(range(1,21,2))\n",
    "n_grid = [{'n_neighbors':k_neighbours}]\n",
    "# apply result to classifier function\n",
    "model = neighbors.KNeighborsClassifier()\n",
    "cv_knn = GridSearchCV(estimator=model,param_grid=n_grid,cv=ms.KFold(n_splits=10))\n",
    "cv_knn.fit(XTrain,YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "11\n"
    }
   ],
   "source": [
    "# Model results (best k)\n",
    "best_k = cv_knn.best_params_['n_neighbors']\n",
    "print(best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n           weights='uniform')"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Train model with best k\n",
    "knnclf = neighbors.KNeighborsClassifier(n_neighbors=best_k)\n",
    "knnclf.fit(XTrain[:,2:4],YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2,\n       1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 0, 2, 1, 0, 0, 0, 0, 2, 2, 1, 2, 2,\n       1])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred = knnclf.predict(XTest[:,2:4])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[12,  0,  0],\n       [ 0, 14,  2],\n       [ 0,  2, 15]])"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(YTest,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       0.88      0.88      0.88        16\n           2       0.88      0.88      0.88        17\n\n   micro avg       0.91      0.91      0.91        45\n   macro avg       0.92      0.92      0.92        45\nweighted avg       0.91      0.91      0.91        45\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(YTest,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is used in the prediction of a discrete outcome\n",
    "and therefore best suited for classification purposes. Logistic\n",
    "regression is in effect another generalised linear model that\n",
    "uses the same basic background as linear regression.\n",
    "However, instead of a continuous dependent variable, the\n",
    "model is regressing for the probability of a (binary) categorical outcome. We can then use these probabilities to\n",
    "obtain class labels for our data observations. In logistic regression, we are interested in determining the\n",
    "probability that an observation belongs to a category (or not)\n",
    "and therefore the conditional mean of the outcome variable. We need to extend the linear regression model to map the outcome variable into that\n",
    "unit interval. In logistic regression, however, the outcome variable can\n",
    "take only two values: Either 0 or 1. This means that instead of following a Gaussian distribution it follows a Bernoulli\n",
    "one. The Bernoulli distribution corresponds to a random\n",
    "variable that takes the value 1 with probability p and 0 with\n",
    "probability q = 1 - p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = pd.read_csv('Data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0  ...         25.38          17.33           184.60      2019.0   \n1  ...         24.99          23.41           158.80      1956.0   \n2  ...         23.57          25.53           152.50      1709.0   \n3  ...         14.91          26.50            98.87       567.7   \n4  ...         22.54          16.67           152.20      1575.0   \n\n   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n0            0.1622             0.6656           0.7119                0.2654   \n1            0.1238             0.1866           0.2416                0.1860   \n2            0.1444             0.4245           0.4504                0.2430   \n3            0.2098             0.8663           0.6869                0.2575   \n4            0.1374             0.2050           0.4000                0.1625   \n\n   symmetry_worst  fractal_dimension_worst  \n0          0.4601                  0.11890  \n1          0.2750                  0.08902  \n2          0.3613                  0.08758  \n3          0.6638                  0.17300  \n4          0.2364                  0.07678  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 32 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "bc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bc = bc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labels\n",
    "X = bc.drop(['diagnosis'],axis=1)\n",
    "X = X.values\n",
    "Y_raw = bc['diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to 0 and 1\n",
    "from sklearn import preprocessing\n",
    "label_enc = preprocessing.LabelEncoder()\n",
    "label_enc.fit(Y_raw)\n",
    "Y = label_enc.transform(Y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test\n",
    "import sklearn.model_selection as cv\n",
    "XTrain,XTest,YTrain,YTest = ms.train_test_split(X,Y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use regularisation in the model and can choose between L1 and L2 penalties\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "pen_val = ['l1','l2']\n",
    "C_val = 2. ** np.arange(-5,10,step=2)\n",
    "grid_s = [{'C':C_val,'penalty':pen_val}]\n",
    "model = LogisticRegression()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "cv_logr = GridSearchCV(estimator=model,param_grid=grid_s,cv=ms.KFold(n_splits=10))\n",
    "# Model fitting\n",
    "cv_logr.fit(XTrain,YTrain)\n",
    "best_c = cv_logr.best_params_['C']\n",
    "best_penalty = cv_logr.best_params_['penalty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=128.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "# Create an instance of the logistic regression model\n",
    "b_clf = LogisticRegression(C=best_c,penalty=best_penalty)\n",
    "b_clf.fit(XTrain,YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9766081871345029\n"
    }
   ],
   "source": [
    "# Predict\n",
    "predict = b_clf.predict(XTest)\n",
    "y_proba = b_clf.predict_proba(XTest)\n",
    "print(b_clf.score(XTest,YTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 3.84883758e-09 -9.31267991e-01  1.47545778e-01 -1.21785996e-01\n  -2.72486306e-03  0.00000000e+00 -3.18103919e+01  1.26021393e+01\n   9.85393362e+01  0.00000000e+00  0.00000000e+00  4.33243499e-02\n  -1.86163550e-01 -6.15026172e-03  1.69921352e-01  0.00000000e+00\n  -6.68164161e+01 -1.94733922e+01  0.00000000e+00  0.00000000e+00\n   0.00000000e+00 -5.48167019e-02  2.55421229e-01  2.84395012e-02\n   2.35265717e-02  0.00000000e+00 -3.55547741e-01  5.83800329e+00\n   5.10734917e+01  2.04216303e+01  0.00000000e+00]]\n"
    }
   ],
   "source": [
    "# coefficients\n",
    "print(b_clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1.00000000e+00 3.94053737e-01 1.15898634e+00 8.85337814e-01\n  9.97278846e-01 1.00000000e+00 1.53081368e-14 2.97193677e+05\n  6.23864074e+42 1.00000000e+00 1.00000000e+00 1.04427655e+00\n  8.30137815e-01 9.93868612e-01 1.18521163e+00 1.00000000e+00\n  9.59398863e-30 3.48990193e-09 1.00000000e+00 1.00000000e+00\n  1.00000000e+00 9.46658653e-01 1.29100532e+00 1.02884776e+00\n  1.02380550e+00 1.00000000e+00 7.00789487e-01 3.43093597e+02\n  1.51682554e+22 7.39607612e+08 1.00000000e+00]]\n"
    }
   ],
   "source": [
    "# odds rastio (exp of coefficients) -> how a unit increase or decrease in a variable affects the odds of having a malignant mass\n",
    "print(np.exp(b_clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9979423868312757\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "fpr,tpr,threshold = roc_curve(YTest,y_proba[:,1])\n",
    "print(auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Classification with Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we used logistic regression to\n",
    "estimate the probability that a data instance belongs to a\n",
    "particular class and, based on that, decide on the label that\n",
    "should be assigned to that instance knew what was meant by probability, i.e. a number between\n",
    "0 and 1 that indicates how likely it is for an event A to occur.\n",
    "We denote the probability of event A as P(A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a traditional standpoint, probability is presented\n",
    "in terms of a frequentist view where data instances are drawn from a repeatable random sample with parameters\n",
    "that remain constant during the repeatable process. These\n",
    "assumptions enable us to determine a frequency with which\n",
    "an event occurs. In contrast, the Bayesian view takes the\n",
    "approach that data instances are observed from a realised\n",
    "sample, and that the parameters are unknown. Since the repeatability of the data sample does not hold under this\n",
    "view, the Bayesian probability is not given in terms of\n",
    "frequencies, but instead it represents a state of knowledge or\n",
    "a state of “belief”.\n",
    "Bayes’ theorem states that the probability of a particular\n",
    "hypothesis is given by both current information (data)\n",
    "and prior knowledge. The prior information may be the\n",
    "outcome of earlier experiments or trials, or even educated\n",
    "guesses drawn from experience. This is the reason why\n",
    "many frequentist practitioners have shunned the Bayesian\n",
    "approach for centuries. Nonetheless, Bayesian statistics has\n",
    "stood the test of time9 and demonstrated its usefulness in \n",
    "many applications. Let us consider the set of all possible\n",
    "events, W, which is our sample space. Event A is a member of\n",
    "the sample space, as is every other event. The probability of the sample space is P(W) = 1. The probability P(A) is given\n",
    "by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/probability_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where |A| denotes the cardinality of A. We show a Venn\n",
    "diagram of this situation in Figure 6.7.a). If |A| were to have\n",
    "equal cardinality to W then the probability of A would be at\n",
    "most 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/probability_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, for this example, the question of real importance\n",
    "is whether for a randomly selected woman, given that the\n",
    "test is positive, what is the probability that she has cancer? In terms of our Venn diagrams (see Figure 6.7.d)) the question\n",
    "above is equivalent to given that we are in region B, what is the\n",
    "probability that we in fact are in region AB?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is what we know as Bayes’ theorem. We call\n",
    "P(A|B) the posterior probability, P(A) the prior probability and P(B|A) the likelihood. Bayes’ theorem can be thought\n",
    "of as a rule that enables us to update our belief about a\n",
    "hypothesis A in light of new evidence B, and so our\n",
    "posterior belief P(A|B) is updated by multiplying our prior belief P(A) by the likelihood P(B|A) that B will occur if A\n",
    "is actually true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/bayes.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian statistics we tend to work along the following\n",
    "steps: 1. We first set out a probability model for an unknown\n",
    "parameter of interest, and include any prior knowledge\n",
    "about the parameter, if available at all. 2. Using the conditional probability of the parameter on observed data, we update our knowledge of this\n",
    "parameter. 3. We then evaluate the fitness of the model to the data and check the conclusions reached based on our assumptions. 4. We may then decide to start over again using the new information gained as our starting prior. The priors we use in the first step above reflect the best\n",
    "approximation to what we are interested in modelling. This information may come from expert information, researcher intuition, previous studies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data_Science      Date                                              Tweet\n62             1  10/10/15  And that is Chapter 3 of \"Data Science and Ana...\n63             1  29/11/15  See sklearn trees with #D3 https://t.co/UYsi0X...\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('Data/Train_QuantumTunnel_Tweets.csv',encoding='utf-8')\n",
    "print(train[62:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub(r\"http\\S+\", \"\", text)\n",
    "import re\n",
    "def tw_preprocess(tw):\n",
    " ptw = re.sub(r\"http\\S+\", \"\", tw)\n",
    " ptw = re.sub(r\"#\",\"\",tw)\n",
    " return ptw   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Tweet'] = train['Tweet'].apply(tw_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectoriser = CountVectorizer(lowercase=True,stop_words='english',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.fit_transform(train['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['networks', 'neural', 'neurons', 'neutrinos', 'new', 'news']"
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "vectoriser.get_feature_names()[1005:1011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "from sklearn import naive_bayes\n",
    "model = naive_bayes.MultinomialNB().fit(X_train,list(train['Data_Science']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.75229358, 0.81481481, 0.77570093])"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "# Cross validation\n",
    "import sklearn.model_selection as ms\n",
    "ms.cross_val_score(naive_bayes.MultinomialNB(),X_train,train['Data_Science'],cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[195,   1],\n       [  0, 128]])"
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(train['Data_Science'],model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0\n 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0\n 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0\n 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1]\n"
    }
   ],
   "source": [
    "# Apply model to test data\n",
    "test = pd.read_csv(\"Data/Test_quantumTunnel_Tweets.csv\",encoding='utf-8')\n",
    "test['Tweet'] = test['Tweet'].apply(tw_preprocess)\n",
    "X_test = vectoriser.transform(test['Tweet'])\n",
    "pred = model.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.67186181 0.02536232 0.22057834 0.00904223 0.23545059 0.64932954\n 0.24365495 0.02631486 0.80339431 0.41317761 0.15908555 0.55246327\n 0.48706062 0.2980012  0.45546653 0.1237307  0.07860627 0.79824994\n 0.10318183 0.86057407 0.98674159 0.84744564 0.73413288 0.80372888\n 0.21819939 0.8599928  0.16104061 0.99629937 0.94303828 0.0494522\n 0.9978514  0.28250415 0.25442652 0.03299886 0.71807345 0.93676266\n 0.41317761 0.07238841 0.75991321 0.24041943 0.02677055 0.96089161\n 0.99501932 0.90805128 0.71807345 0.99564784 0.31225906 0.83158785\n 0.89144906 0.40513504 0.5058674  0.55246327 0.2980012  0.6293572\n 0.74820768 0.90209262 0.99146065 0.14717961 0.80934363 0.28250415\n 0.41317761 0.11100037 0.01537336 0.08851676 0.29152604 0.17230772\n 0.41317761 0.22057834 0.6293572  0.80934363 0.00341011 0.35982869\n 0.97556576 0.00347304 0.00757539 0.43288831 0.70490559 0.9110121\n 0.24365495 0.0537064  0.75437488 0.74820768 0.00888553 0.74468276\n 0.03409679 0.07048788 0.03068052 0.1722538  0.29488939 0.11151595\n 0.62990252 0.10213228 0.25442652 0.82078617 0.99548073 0.38115744\n 0.45024697 0.25442652 0.54757366 0.45024697 0.03970616 0.53818186\n 0.99961763 0.98357675 0.02535298 0.92472702 0.23583235 0.87064417\n 0.16448608 0.00949672 0.58868506 0.24365495 0.48706062 0.6293572\n 0.48706062 0.02759419 0.03237411 0.22057834 0.07603911 0.74376218\n 0.16263738 0.5058674  0.10482531 0.17478304 0.2712068  0.83307663\n 0.10482531 0.36406624 0.00751043 0.04372153 0.96734895 0.20378176\n 0.19184542 0.02181819 0.40089178 0.45426736 0.10213228 0.98972117\n 0.25442652 0.94303828 0.20283177 0.16263738 0.06678232 0.16263738\n 0.53043123 0.41317761 0.57994467 0.0537064  0.41317761 0.06868918\n 0.97809625 0.35427463 0.2807487  0.25442652 0.6293572  0.21114372\n 0.05839983 0.0823537  0.06388944 0.6293572  0.71807345 0.76332136\n 0.99586295]\n"
    }
   ],
   "source": [
    "# probability assigned to each tweet\n",
    "pred_probs = model.predict_proba(X_test)[:,1]\n",
    "print(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}