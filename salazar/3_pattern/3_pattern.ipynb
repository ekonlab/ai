{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Recognising Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In feature analysis, we are said to recognise an object by\n",
    "considering the constituent parts, or features, of the object.\n",
    "We then assemble them together to determine what the\n",
    "object is. For example, we know that a cat is a small fury If we know what a cat looks like,\n",
    "we can recognise other cats. animal with triangular ears, long whiskers and playful\n",
    "claws. When we see a cat we recognise it for what it is\n",
    "because it satisfies these (admittedly simplified) rules.\n",
    "The field of pattern recognition is thus interested in the\n",
    "systematic detection of regularities in a dataset, based on\n",
    "the use of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Artificial Intelligence and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning can be seen as a Machine learning is a subfield of\n",
    "artificial intelligence focussed on\n",
    "improving the performance of an\n",
    "intelligent agent.\n",
    "\n",
    "subfield of artificial intelligence.\n",
    "machine\n",
    "learning is interested in studying the methods that can be\n",
    "used to improve the performance of an intelligent agent\n",
    "over time, based on stimuli from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Learning, Predicting and Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of machine learning algorithms\n",
    "involves the analysis of data that could be employed in the\n",
    "improvement (learning) of the agent (model) and\n",
    "subsequently using the results to make predictions about\n",
    "quantities of interest or making decisions in the face of uncertainty.\n",
    "It is important to bear in mind that machine learning is interested in\n",
    "regularities and patterns in data. interested in the regularities or patterns of the data in order\n",
    "to provide predictive and/or classifying power. This is not\n",
    "necessarily the same as causality.\n",
    "Machine learning tasks are traditionally divided into two\n",
    "camps: Predictive or supervised learning and descriptive\n",
    "or unsupervised learning. \n",
    "A teacher\n",
    "that knows what a cat looks like will present the pupil\n",
    "with several training images of cats and other animals,\n",
    "and the pupil is expected to use the features or attributes of\n",
    "the images presented to learn what a cat looks like. The\n",
    "teacher will have provided a label to each of the images as being of cats or not. In the testing part, the teacher will\n",
    "present images of various kinds of animals, and the pupil is\n",
    "expected to classify which ones show a friendly feline face.\n",
    "In machine learning parlance we talk about supervised\n",
    "learning when we are interested in learning a mapping\n",
    "from the input to output with the help of labelled sets\n",
    "of input-output pairs. predictions based on the data that we see and thus apply\n",
    "generalisations.\n",
    "Each input has a number of features that can be represented\n",
    "in terms of an N-dimensional vector that will help in the\n",
    "task of learning the label of each of the training examples.\n",
    "In unsupervised\n",
    "learning. In this case, following our example of the\n",
    "teacher-pupil situation, the teacher takes a Montessori-style\n",
    "approach and lets the pupil develop, on her own, a rule\n",
    "about what a cat (or any other animal of the pupil’s\n",
    "preference) looks like, without providing any hints or labels\n",
    "to the learner.\n",
    "In this case, from a machine learning point of view, there are no input-output pairs. Instead, we only have the\n",
    "unlabelled inputs and their associated N-dimensional\n",
    "feature vectors, without being told the kind of pattern that\n",
    "we must look for. that an unsupervised\n",
    "learning task may enable us to assign labels to those inputs\n",
    "and thus open the door to the use of predictive or\n",
    "supervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Machine Learning and Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms are suitable to the solution of problems\n",
    "encountered in the data science and analytics workflow\n",
    "where we are interested in deriving valuable insights from\n",
    "data.\n",
    "The\n",
    "improvement in learning comes from generalising regular\n",
    "patterns in the training data to be able to say something\n",
    "about unobserved data points. We should therefore be\n",
    "careful not to obtain a model that “memorises” the data,\n",
    "also known as overfitting. We can avoid this by employing techniques such as regularisation and cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unprocessed data can thus be thought of as the raw\n",
    "material that can be filtered and prepared to obtain the\n",
    "insights desired. We need\n",
    "to be able to think through the available independent\n",
    "variables or features (ingredients) that will be included in\n",
    "the model (recipe).In some cases using the unprocessed, raw data may be\n",
    "suitable. However, in many cases it is preferable to create\n",
    "new features that synthesise important signals spread out in\n",
    "the raw data. This process is known as feature selection where not only should we consider the the features readily\n",
    "available, but also the creation and extraction of new\n",
    "features and even the elimination of some variables too.\n",
    "A common way to create new features is via\n",
    "mathematical transformations that make the variables suitable for exploitation by a particular algorithm. For\n",
    "instance, many algorithms rely on features having a linear\n",
    "relationship, and finding a transformation that renders\n",
    "nonlinear features to be represented as being linear in a\n",
    "different feature space is definitely worth considering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Bias, Variance and Regularisation: A Balancing Act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine\n",
    "learning algorithms enable us to exploit the regularities in\n",
    "the data. Our task is therefore to generalise those\n",
    "regularities and apply them to new data points that have not been observed. This is called generalisation, and we are\n",
    "interested in minimising the so-called generalisation error, i.e.\n",
    "a measure of how well our model performs against unseen\n",
    "data. If we were able to create an algorithm that is able to recall\n",
    "the exact noise in the training data, we would be able to bring our training error down to zero. That sounds great and we would be very happy until we receive a new batch\n",
    "of data to test our model. It is quite likely that the\n",
    "performance of the model is not as good as a zero\n",
    "generalisation error would have us believe. We have ended\n",
    "up with an overfit model: We would be able to describe the\n",
    "noise in our data instead of uncovering a relationship, given\n",
    "the variance in our data.\n",
    "The key is to maintain a balance between the propensity of\n",
    "our model to learn the wrong thing, i.e the bias, and the\n",
    "sensitivity to small fluctuations in the data, i.e. the variance. The key is to maintain a balance\n",
    "between bias and variance. In the ideal case scenario we are interested in obtaining\n",
    "a model that encapsulates patterns in the training data,\n",
    "and that at the same time generalises well to data not yet\n",
    "observed. As you can imagine, the tension between both\n",
    "tasks means that we cannot do both equally well and a\n",
    "trade-off must be found in order to represent the training\n",
    "data well (high variance) without risking overfitting (high\n",
    "bias).\n",
    "High-bias models typically produce simpler models that do\n",
    "not overfit and in those cases the danger is that of\n",
    "underfitting. Models with low-bias are typically more complex and that complexity enables us to represent the\n",
    "training data in a more accurate way. The danger here is\n",
    "that the flexibility provided by higher complexity may end\n",
    "up representing not only a relationship in the data but also\n",
    "the noise. Another way of portraying the bias-variance\n",
    "trade-off is in terms of complexity v simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tension between bias and variance, simplicity and\n",
    "complexity, or underfitting and overfitting is an area in the\n",
    "data science and analytics process that can be closer to a\n",
    "craft than a fixed rule. The main challenge is that not only is\n",
    "each dataset different, but also there are data points that we have not yet seen at the moment of constructing the model.\n",
    "Instead, we are interested in building a strategy that enables\n",
    "us to tell something about data from the sample used in\n",
    "building the model. In order to prevent overfitting it is possible to introduce\n",
    "ways to penalise our models for complexity by adding extra\n",
    "constraints such as smoothness, or requiring bounds in the\n",
    "norm of the vector space we are working on. This process is known as regularisation, and the effects of y. the penalty introduced can be adjusted with the use of the\n",
    "so-called regularisation hyperparameter, l. Regularisation is the process\n",
    "of introducing to our model a\n",
    "penalty for complexity. Regularisation can then be employed to fine-tune the\n",
    "complexity of the model in question. Some typical penalty methods that are introduced for\n",
    "regularisation are the L1 and L2 norms that we will discuss\n",
    "in the following section. In Section 3.12 we will touch upon\n",
    "how the hyperparameter l can be tuned with the use of\n",
    "cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Some Useful Measures: Distance and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built a set of models based on the training\n",
    "data we have, it is important to distinguish a good\n",
    "performing model against a less good one. So, how do we ascertain that a model is good enough for our purposes?\n",
    "The answer is that we need to evaluate the models with the\n",
    "aid of a scoring or objective function. The\n",
    "performance of a model will therefore depend on various\n",
    "factors such as the distribution of classes, the cost of\n",
    "misclassification, the size of the dataset, the sampling\n",
    "methods used to obtain the data, or even the range of values\n",
    "in the selected features. In general model evaluation can be posed as a constrained\n",
    "optimisation problem given an objective function. The aim can then be presented as the problem of finding a set of\n",
    "parameters that minimises that objective function. This is a\n",
    "very useful way to tackle the problem as the evaluation\n",
    "measure can be included as part of the objective function\n",
    "itself. For example, consider the case where we are interested in finding the best line of fit given a number of\n",
    "data points: A perfect fit would be found in the case where the data points align flawlessly in a straight line. We can evaluate how\n",
    "well a line fits the data when we take into account the\n",
    "difference between the location of a point and its\n",
    "corresponding prediction as obtained from the model. If we\n",
    "minimise that distance then we can evaluate and compare\n",
    "various calculated predictions. This particular evaluation measure used in regression analysis is known as the sum of\n",
    "squared residuals (SSR) and we will discuss it in more detail\n",
    "in Chapter 4. In regression, the minimisation\n",
    "of the sum of squares error is a\n",
    "typical evaluation measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the concept of distance arises naturally as\n",
    "a way to express the evaluation problem, and indeed a\n",
    "number of conventional evaluation procedures rely on\n",
    "measures of distance. Consider the points A and B in a two dimensional space shown in Figure 3.1. Point A has\n",
    "coordinates p(p1, p2) and point B has coordinates q(q1, q2).\n",
    "We are interested in calculating the distance between these\n",
    "two points. This can be achieved in different ways and we\n",
    "are familiar with some of these, such as the Euclidean and\n",
    "the Manhattan distances. Euclidean distance: This corresponds to the ordinary\n",
    "distance calculated using the straight line that joins\n",
    "points A and B; in two dimensions it corresponds to the\n",
    "distance given by the Pythagorean theorem. Given the coordinates of each of the two points in question we can obtain the distance between A and B as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/euclidean_distance.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/euclidean_distance_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manhattan distance: It is easy to see why this distance\n",
    "measure gets this name if we think of the distance that a\n",
    "yellow cab would cover while travelling along the streets in Manhattan. Apart from Broadway, the cab cannot\n",
    "move diagonally in the street-avenue grid. Instead, it can\n",
    "only move North-South and East-West. In the case of\n",
    "points A and B in Figure 3.1, the Manhattan distance is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manhattan Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/manhattan_distance_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/manhattan_distance_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distance is zero we can argue that the\n",
    "two points are effectively the same one, or at the very least\n",
    "similar to one another. This idea of similarity is therefore another useful tool in the development of evaluation\n",
    "measures, particularly in the case where features are not\n",
    "inherently amenable to being placed in a geometric space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity: This similarity measure is commonly\n",
    "used in text mining tasks, for example. In these cases the\n",
    "words in the documents that comprise the corpora to be\n",
    "mined correspond to our data features. The features can\n",
    "be arranged into vectors and our task is to determine if\n",
    "any two documents are similar or not. Cosine similarity\n",
    "is based on the calculation of the dot product of the\n",
    "feature vectors. It is effectively a measure of the angle q\n",
    "between the vectors: If q = 0, then cos q is 1 and the two\n",
    "vectors are said to be similar. For any other value of q the\n",
    "cosine similarity will be less than 1. The cosine similarity\n",
    "of vectors v1 and v2 is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/cosine_similarity.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity: The Jaccard similarity measure provides us with a way to compare unordered collections\n",
    "of objects, i.e. sets. We define the Jaccard similarity in\n",
    "terms of the elements that are common to the sets in\n",
    "question. Consider two sets A and B with cardinalities\n",
    "|A| and |B|. The common elements of both sets are given\n",
    "by the intersection A \\ B. In order to give us an idea of the\n",
    "relative size of the intersection compared to the sets, we divide the former by the union of the sets. This can be\n",
    "expressed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/jaccard_similarity.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of document similarity for example, two\n",
    "identical documents will have a Jaccard similarity of 1 and those completely dissimilar a value of 0.\n",
    "Intermediate values correspond to various degrees of\n",
    "similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Beware the Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been referring to data features as an integral\n",
    "part of the ingredients we will use with our machine\n",
    "learning algorithms. For a single feature we have a one-dimensional space, two\n",
    "features can be represented in two dimensions. It follows that as we increase the number of features, the\n",
    "number of dimensions that our model must include is increased too. Not only that, but we will also increase\n",
    "the amount of information required to describe the data\n",
    "instances, and therefore the model. The realisation that the number\n",
    "of data points required to sample a space grows\n",
    "exponentially with the dimensionality of the space is usually\n",
    "called the curse of dimensionality. The curse of dimensionality becomes more apparent in\n",
    "instances where we have datasets with a number of features\n",
    "much larger than the number of data points. We can see\n",
    "why this is the case when we consider the calculation of the distance between data points in spaces with increasing\n",
    "dimensionality. Avoiding the curse of\n",
    "dimensionality can be done by increasing the amount of\n",
    "data, but even before going down that route it is worth\n",
    "considering if the features used are indeed a suitable\n",
    "collection. In that respect, apart from a careful feature selection process,\n",
    "we can also reduce the dimensionality of the problem by\n",
    "transforming the data from a higher-dimensional space into\n",
    "a space with fewer dimensions as it is the case with Principal Component Analysis (PCA). We will discuss this\n",
    "technique in Chapter 8. As for avoiding overfitting, in \n",
    "Section 3.12 we will discuss the ideas behind\n",
    "cross-validation. But first we need to make a stop to talk\n",
    "about Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data[0:6,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out our modelling and the result can be used to\n",
    "classify any new iris flower we encounter based on the\n",
    "4 measurements (features) used. However, how do we\n",
    "know how well (or how badly) our model performs? We would have to wait until we get new data not seen by the\n",
    "model. What is more, we must remember that we build a model\n",
    "because we are interested in using it effectively. This means\n",
    "that we should care about its performance with new unseen\n",
    "data and therefore a way to assess this is with error rates.\n",
    "One way to tackle this problem is to prepare two\n",
    "independent datasets from the original one: Training set: This is the data that the model will see and\n",
    "it is used to determine the parameters of the model. Testing set: We can think of this as “new” data. The model has not encountered it yet and it will enable us\n",
    "to measure the performance of the model built with the\n",
    "training set. In some cases instead of partitioning the data into two sets,\n",
    "it is divided into three. The third component is called the\n",
    "validation set and it is used for tuning the model. All three parts need to be representative of the data that will be used\n",
    "with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(iris.data,iris.target,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in making accurate and useful\n",
    "predictions, we need to ensure that any models we create\n",
    "generalise well to unseen data. The parameters that we obtain with the use of a\n",
    "single training dataset may end up reflecting the particular\n",
    "way in which the data split was performed. The solution to this problem is straightforward: We can use\n",
    "statistical sampling to get more accurate measurements. This process is usually referred to as cross-validation. Cross-\n",
    "validation improves statistical efficiency by performing repeated splitting of data into training and validation sets, and re-performing model training and evaluation every time. The aim of cross-validation is to use a dataset to validate the model during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common cross-validation technique is the k-fold procedure: The original data is divided into k equal sets.\n",
    "From the k subsets, a single partition is kept for validating\n",
    "the model, and k-1 subsets are used for training. The process is then repeated k times, using one by one each\n",
    "of the k subsets for validation. We will therefore have a\n",
    "total of k trained models. The results of each of the folds\n",
    "can be combined, for instance by averaging, to obtain a single estimation of the out-of-sample error. Cross-validation is a useful and straightforward way to get a more accurate estimate of the out-of-sample error, and\n",
    "at the same time a more efficient use of data than a single training/testing split. This is because each record in the\n",
    "dataset is used in both training and validating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation can also be useful in feature and model\n",
    "selection procedures. For example, it can be used for tuning\n",
    "the regularisation parameter l introduced in Section 3.7: We split out training data and train a model for a fixed value of\n",
    "l. We can then test it on the remaining subsets and repeat\n",
    "this procedure while varying l. Finally, we select the best l\n",
    "that minimises our measure of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
