{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Recognising Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In feature analysis, we are said to recognise an object by\n",
    "considering the constituent parts, or features, of the object.\n",
    "We then assemble them together to determine what the\n",
    "object is. For example, we know that a cat is a small fury If we know what a cat looks like,\n",
    "we can recognise other cats. animal with triangular ears, long whiskers and playful\n",
    "claws. When we see a cat we recognise it for what it is\n",
    "because it satisfies these (admittedly simplified) rules.\n",
    "The field of pattern recognition is thus interested in the\n",
    "systematic detection of regularities in a dataset, based on\n",
    "the use of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Artificial Intelligence and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning can be seen as a Machine learning is a subfield of\n",
    "artificial intelligence focussed on\n",
    "improving the performance of an\n",
    "intelligent agent.\n",
    "\n",
    "subfield of artificial intelligence.\n",
    "machine\n",
    "learning is interested in studying the methods that can be\n",
    "used to improve the performance of an intelligent agent\n",
    "over time, based on stimuli from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Learning, Predicting and Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of machine learning algorithms\n",
    "involves the analysis of data that could be employed in the\n",
    "improvement (learning) of the agent (model) and\n",
    "subsequently using the results to make predictions about\n",
    "quantities of interest or making decisions in the face of uncertainty.\n",
    "It is important to bear in mind that machine learning is Machine learning is interested in\n",
    "regularities and patterns in data. interested in the regularities or patterns of the data in order\n",
    "to provide predictive and/or classifying power. This is not\n",
    "necessarily the same as causality.\n",
    "Machine learning tasks are traditionally divided into two\n",
    "camps: Predictive or supervised learning and descriptive\n",
    "or unsupervised learning. \n",
    "A teacher\n",
    "that knows what a cat looks like will present the pupil\n",
    "with several training images of cats and other animals,\n",
    "and the pupil is expected to use the features or attributes of\n",
    "the images presented to learn what a cat looks like. The\n",
    "teacher will have provided a label to each of the images as being of cats or not. In the testing part, the teacher will\n",
    "present images of various kinds of animals, and the pupil is\n",
    "expected to classify which ones show a friendly feline face.\n",
    "In machine learning parlance we talk about supervised\n",
    "learning when we are interested in learning a mapping\n",
    "from the input to output with the help of labelled sets\n",
    "of input-output pairs. predictions based on the data that we see and thus apply\n",
    "generalisations.\n",
    "Each input has a number of features that can be represented\n",
    "in terms of an N-dimensional vector that will help in the\n",
    "task of learning the label of each of the training examples.\n",
    "In unsupervised\n",
    "learning. In this case, following our example of the\n",
    "teacher-pupil situation, the teacher takes a Montessori-style\n",
    "approach and lets the pupil develop, on her own, a rule\n",
    "about what a cat (or any other animal of the pupil’s\n",
    "preference) looks like, without providing any hints or labels\n",
    "to the learner.\n",
    "In this case, from a machine learning point of view, there are no input-output pairs. Instead, we only have the\n",
    "unlabelled inputs and their associated N-dimensional\n",
    "feature vectors, without being told the kind of pattern that\n",
    "we must look for. that an unsupervised\n",
    "learning task may enable us to assign labels to those inputs\n",
    "and thus open the door to the use of predictive or\n",
    "supervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Machine Learning and Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms are suitable to the solution of problems\n",
    "encountered in the data science and analytics workflow\n",
    "where we are interested in deriving valuable insights from\n",
    "data.\n",
    "The\n",
    "improvement in learning comes from generalising regular\n",
    "patterns in the training data to be able to say something\n",
    "about unobserved data points. We should therefore be\n",
    "careful not to obtain a model that “memorises” the data,\n",
    "also known as overfitting. We can avoid this by employing techniques such as regularisation and cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unprocessed data can thus be thought of as the raw\n",
    "material that can be filtered and prepared to obtain the\n",
    "insights desired. We need\n",
    "to be able to think through the available independent\n",
    "variables or features (ingredients) that will be included in\n",
    "the model (recipe).In some cases using the unprocessed, raw data may be\n",
    "suitable. However, in many cases it is preferable to create\n",
    "new features that synthesise important signals spread out in\n",
    "the raw data. This process is known as feature selection where not only should we consider the the features readily\n",
    "available, but also the creation and extraction of new\n",
    "features and even the elimination of some variables too.\n",
    "A common way to create new features is via\n",
    "mathematical transformations that make the variables suitable for exploitation by a particular algorithm. For\n",
    "instance, many algorithms rely on features having a linear\n",
    "relationship, and finding a transformation that renders\n",
    "nonlinear features to be represented as being linear in a\n",
    "different feature space is definitely worth considering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Bias, Variance and Regularisation: A Balancing Act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine\n",
    "learning algorithms enable us to exploit the regularities in\n",
    "the data. Our task is therefore to generalise those\n",
    "regularities and apply them to new data points that have not been observed. This is called generalisation, and we are\n",
    "interested in minimising the so-called generalisation error, i.e.\n",
    "a measure of how well our model performs against unseen\n",
    "data. If we were able to create an algorithm that is able to recall\n",
    "the exact noise in the training data, we would be able to bring our training error down to zero. That sounds great and we would be very happy until we receive a new batch\n",
    "of data to test our model. It is quite likely that the\n",
    "performance of the model is not as good as a zero\n",
    "generalisation error would have us believe. We have ended\n",
    "up with an overfit model: We would be able to describe the\n",
    "noise in our data instead of uncovering a relationship, given\n",
    "the variance in our data.\n",
    "The key is to maintain a balance between the propensity of\n",
    "our model to learn the wrong thing, i.e the bias, and the\n",
    "sensitivity to small fluctuations in the data, i.e. the variance. The key is to maintain a balance\n",
    "between bias and variance. In the ideal case scenario we are interested in obtaining\n",
    "a model that encapsulates patterns in the training data,\n",
    "and that at the same time generalises well to data not yet\n",
    "observed. As you can imagine, the tension between both\n",
    "tasks means that we cannot do both equally well and a\n",
    "trade-off must be found in order to represent the training\n",
    "data well (high variance) without risking overfitting (high\n",
    "bias).\n",
    "High-bias models typically produce simpler models that do\n",
    "not overfit and in those cases the danger is that of\n",
    "underfitting. Models with low-bias are typically more complex and that complexity enables us to represent the\n",
    "training data in a more accurate way. The danger here is\n",
    "that the flexibility provided by higher complexity may end\n",
    "up representing not only a relationship in the data but also\n",
    "the noise. Another way of portraying the bias-variance\n",
    "trade-off is in terms of complexity v simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tension between bias and variance, simplicity and\n",
    "complexity, or underfitting and overfitting is an area in the\n",
    "data science and analytics process that can be closer to a\n",
    "craft than a fixed rule. The main challenge is that not only is\n",
    "each dataset different, but also there are data points that we have not yet seen at the moment of constructing the model.\n",
    "Instead, we are interested in building a strategy that enables\n",
    "us to tell something about data from the sample used in\n",
    "building the model. In order to prevent overfitting it is possible to introduce\n",
    "ways to penalise our models for complexity by adding extra\n",
    "constraints such as smoothness, or requiring bounds in the\n",
    "norm of the vector space we are working on. This process is known as regularisation, and the effects of y. the penalty introduced can be adjusted with the use of the\n",
    "so-called regularisation hyperparameter, l. Regularisation is the process\n",
    "of introducing to our model a\n",
    "penalty for complexity. Regularisation can then be employed to fine-tune the\n",
    "complexity of the model in question. Some typical penalty methods that are introduced for\n",
    "regularisation are the L1 and L2 norms that we will discuss\n",
    "in the following section. In Section 3.12 we will touch upon\n",
    "how the hyperparameter l can be tuned with the use of\n",
    "cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Some Useful Measures: Distance and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built a set of models based on the training\n",
    "data we have, it is important to distinguish a good\n",
    "performing model against a less good one. So, how do we ascertain that a model is good enough for our purposes?\n",
    "The answer is that we need to evaluate the models with the\n",
    "aid of a scoring or objective function. The\n",
    "performance of a model will therefore depend on various\n",
    "factors such as the distribution of classes, the cost of\n",
    "misclassification, the size of the dataset, the sampling\n",
    "methods used to obtain the data, or even the range of values\n",
    "in the selected features. In general model evaluation can be posed as a constrained\n",
    "optimisation problem given an objective function. The aim can then be presented as the problem of finding a set of\n",
    "parameters that minimises that objective function. This is a\n",
    "very useful way to tackle the problem as the evaluation\n",
    "measure can be included as part of the objective function\n",
    "itself. For example, consider the case where we are interested in finding the best line of fit given a number of\n",
    "data points: A perfect fit would be found in the case where the data points align flawlessly in a straight line. We can evaluate how\n",
    "well a line fits the data when we take into account the\n",
    "difference between the location of a point and its\n",
    "corresponding prediction as obtained from the model. If we\n",
    "minimise that distance then we can evaluate and compare\n",
    "various calculated predictions. This particular evaluation measure used in regression analysis is known as the sum of\n",
    "squared residuals (SSR) and we will discuss it in more detail\n",
    "in Chapter 4. In regression, the minimisation\n",
    "of the sum of squares error is a\n",
    "typical evaluation measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the concept of distance arises naturally as\n",
    "a way to express the evaluation problem, and indeed a\n",
    "number of conventional evaluation procedures rely on\n",
    "measures of distance. Consider the points A and B in a two dimensional space shown in Figure 3.1. Point A has\n",
    "coordinates p(p1, p2) and point B has coordinates q(q1, q2).\n",
    "We are interested in calculating the distance between these\n",
    "two points. This can be achieved in different ways and we\n",
    "are familiar with some of these, such as the Euclidean and\n",
    "the Manhattan distances. Euclidean distance: This corresponds to the ordinary\n",
    "distance calculated using the straight line that joins\n",
    "points A and B; in two dimensions it corresponds to the\n",
    "distance given by the Pythagorean theorem. Given the coordinates of each of the two points in question we can obtain the distance between A and B as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](euclidean_distance.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](euclidean_distance_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}