{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Decision Trees and Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Hiearchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is an unsupervised learning task whose goal is to build a\n",
    "hierarchy of data groups. The hierarchy can be built from\n",
    "the “bottom-up”. In this case each data instance starts in its\n",
    "own cluster, and we successively merge these clusters as we\n",
    "go up in the levels of the hierarchy. This is known as\n",
    "agglomerative clustering. In contrast, divisive clustering\n",
    "takes the opposite approach, starting with all data instances\n",
    "and performing splits as we go down the levels of the\n",
    "hierarchy. The results of hierarchical clustering are presented in a tree-like structure called a dendrogram. Clustering relies on the existence of a similarity\n",
    "measure among data instances. In a dendrogram, data\n",
    "points are joined together from the most similar (closest) to\n",
    "the most different (further apart). In hierarchical clustering we only need one thing: A similarity measure among groups of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}