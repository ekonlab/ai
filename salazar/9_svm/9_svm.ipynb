{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Support Vector Machines and Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have concentrated so far in extracting features from\n",
    "our existing data to obtain a representation using fewer dimensions while retaining as much information as possible.\n",
    "A different approach is to transform the data in such a way\n",
    "that our learning tasks look easier to be carried out than in\n",
    "the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sense, this is what we have done when applying a\n",
    "logarithmic transformation to the mammals dataset in\n",
    "Section 4.5: In the transformed feature space, the relationship between a mammalâ€™s body and brain is better\n",
    "approximated by a straight line than when using the\n",
    "untransformed features. Once we have carried out the\n",
    "learning task, we need to invert the transformation to the\n",
    "original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector machine is a binary linear classifier\n",
    "where the classification boundary is built in such a manner\n",
    "as to minimise the generalisation error in our task. Unlike\n",
    "other classifiers we have discussed, the support vector machine boundary is obtained using geometrical reasoning\n",
    "instead of algebraic. With that in mind, the generalisation\n",
    "error is associated with the geometrical notion of a margin,\n",
    "which can be defined as the region along the classification\n",
    "boundary that is free of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that manner, a support vector machine (SVM) has the\n",
    "goal of discriminating among classes using a linear decision\n",
    "boundary that has the largest margin, giving rise to the so-called maximum margin hyperplane or (MMH). Having\n",
    "the maximum margin is equivalent to minimising the\n",
    "generalisation error. This is because using the MMH as the\n",
    "classification boundary minimises the probability that a \n",
    "small perturbation in the position of a data point results in a\n",
    "classification error. Intuitively, it is easy to see that a wider\n",
    "margin results in having better defined and separate classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/svm_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel is a function K(x, y) whose arguments x and y\n",
    "can be real numbers, vectors, functions, etc. It is effectively\n",
    "a map between these arguments and a real value. The\n",
    "operation is independent of the order of the arguments. \n",
    "are familiar with at least one such kernel: The well-known\n",
    "vector product. We This means that K(x, y) = K(y, x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/svm_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a choice of kernels to use. Some of the more\n",
    "popular ones include:\n",
    "- Linear \n",
    "- Polynomial\n",
    " - Gaussian\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Populating the interactive namespace from numpy and matplotlib\n"
    }
   ],
   "source": [
    "# SVM Regression\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "mammals = pd.read_csv('Data/mammals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = mammals[['body']].values\n",
    "brain = mammals[['brain']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "svm_lm = svm.SVR(kernel='linear',C=1e1) # regression for linear kernel\n",
    "svm_rbf = svm.SVR(kernel='rbf',C=1e1) # gaussian kernel\n",
    "logfit = LinearRegression().fit(np.log(body),np.log(brain)) # regression with log transf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammals['log_regr'] = np.exp(logfit.predict(np.log(body)))\n",
    "#mammals['linear_svm'] = np.exp(svm_lm.predict(np.log(body)))\n",
    "#mammals['rbf_svm'] = np.exp(rbf_svm.predict(np.log(body)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n         normalize=False)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "logfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classification\n",
    "wine = pd.read_csv('Data/wine.csv')\n",
    "X = wine.drop(['Wine'],axis=1).values\n",
    "Y = wine['Wine'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = wine[['Alcohol','Color.int']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\n",
    "XTrain,XTest,YTrain,YTest = ms.train_test_split(X1,Y,test_size=0.3,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC classifier with gaussian kernel\n",
    "from sklearn import svm\n",
    "SVMclassifier = svm.SVC()\n",
    "# gridsearch\n",
    "Cval = 2.**np.arange(-1,1.2,step=0.2)\n",
    "n_grid = [{'C':Cval}]\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "cv_svc = GridSearchCV(estimator=SVMclassifier ,param_grid=n_grid,cv=ms.KFold(n_splits=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "cv_svc.fit(XTrain,YTrain)\n",
    "best_c = cv_svc.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.7411011265922478\n"
    }
   ],
   "source": [
    "print(best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC(C=1.7411011265922478, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n  shrinking=True, tol=0.001, verbose=False)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "svc_clf = svm.SVC(C=best_c)\n",
    "svc_clf.fit(XTrain,YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           1       0.85      0.69      0.76        16\n           2       0.92      0.92      0.92        24\n           3       0.76      0.93      0.84        14\n\n   micro avg       0.85      0.85      0.85        54\n   macro avg       0.84      0.84      0.84        54\nweighted avg       0.86      0.85      0.85        54\n\n"
    }
   ],
   "source": [
    "y_p = svc_clf.predict(XTest)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_p,YTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           1       0.92      0.71      0.80        17\n           2       0.96      0.96      0.96        24\n           3       0.76      1.00      0.87        13\n\n   micro avg       0.89      0.89      0.89        54\n   macro avg       0.88      0.89      0.88        54\nweighted avg       0.90      0.89      0.89        54\n\n"
    }
   ],
   "source": [
    "C = best_c\n",
    "svc = svm.SVC(kernel='linear',C=C).fit(XTrain,YTrain)\n",
    "y_p = svc.predict(XTest)\n",
    "print(metrics.classification_report(y_p,YTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           1       0.85      0.69      0.76        16\n           2       0.92      0.92      0.92        24\n           3       0.76      0.93      0.84        14\n\n   micro avg       0.85      0.85      0.85        54\n   macro avg       0.84      0.84      0.84        54\nweighted avg       0.86      0.85      0.85        54\n\n"
    }
   ],
   "source": [
    "C = best_c\n",
    "rbf_svc = svm.SVC(kernel='rbf',C=C).fit(XTrain,YTrain)\n",
    "y_p = rbf_svc.predict(XTest)\n",
    "print(metrics.classification_report(y_p,YTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           1       0.92      0.75      0.83        16\n           2       0.92      0.96      0.94        23\n           3       0.76      0.87      0.81        15\n\n   micro avg       0.87      0.87      0.87        54\n   macro avg       0.87      0.86      0.86        54\nweighted avg       0.88      0.87      0.87        54\n\n"
    }
   ],
   "source": [
    "C = best_c\n",
    "poly_svc = svm.SVC(kernel='poly',degree=3,C=C).fit(XTrain,YTrain)\n",
    "y_p = poly_svc.predict(XTest)\n",
    "print(metrics.classification_report(y_p,YTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           1       0.85      0.69      0.76        16\n           2       0.92      0.92      0.92        24\n           3       0.76      0.93      0.84        14\n\n   micro avg       0.85      0.85      0.85        54\n   macro avg       0.84      0.84      0.84        54\nweighted avg       0.86      0.85      0.85        54\n\n"
    }
   ],
   "source": [
    "C = best_c\n",
    "lin_svc = svm.SVC(C=C).fit(XTrain,YTrain)\n",
    "y_p = lin_svc.predict(XTest)\n",
    "print(metrics.classification_report(y_p,YTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}