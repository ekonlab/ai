---
title: "Tidymodels"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

Tidymodels: <https://www.tmwr.org/>.

Models are mathematical tools that can describe a system and capture relationships in the data given to them. The utility of a model hinges on its ability to be reductive.

### 1.1. Types of models

#### Descriptive models

The purpose of a descriptive model is to describe or illustrate characteristics of some data. The analysis might have no other purpose than to visually emphasize some trend or artifact in the data. An example can be a LOESS (locally estimated smoothing model) model, that consists on a smooth and flexible regression model. This model is used to discover potential ways to represent a variable.

#### Inferential models

The goal of an inferential model is to produce a decision for a research question or to test a specific hypothesis. It aims to make some statement of truth regarding a predefined conjecture or idea. In many cases, a qualitative statement is produced (e.g, a difference was "statistically significant").

For example, in a clinical trial, the goal might be to provide confirmation that a new therapy does a better job in prolonging life than an alternative, like an existing therapy or no treatment. If the clinical endpoint was related to survival of a patient, the null hypothesis might be that the two therapeutic groups have equal median survival times with the alternative hypothesis being that the new therapy has higher median survival. If this trial were evaluated using the traditional null hypothesis significance testing, a p-value would be produced using some predefined methodology. Small values of the p-value indicate that there is evidence that the new therapy does help patients live longer. If not, the trial would conclude that there is a failure to show such a difference, which could be due to a number of reasons.

Inferential techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. There tends to be a delayed feedback loop in understanding how well the data matches the assumptions. In the clinical trial example, if statistical significance indicate that the new therapy should be available for patients to use, it still may be years before it is used in the field and enough data are generated for an independent assessment of whether the original statistical analysis led to the appropriate decision.

#### Predictive models

They model data to produce the most accurate prediction possible for new data. The primary goal is that the predicted values have the highest possible fidelity to the true value of the new data. A simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month. For this type of model, the problem type is one of estimation rather than inference. 

### 1.2 Terminology

Many models can be categorised as being supervised or unsupervised. Unsupervised models are those that learn patterns, clusters or other characteristics of the data but lack an outcome variable. PCA, cluster or autoencoders are examples of unsupervised models.They are used to understand relationships between variables or sets of variables without an explicit relationship between variables and an outcome. Supervised models are those that have an outcome variable. Within supervised models, there are two main sub-categories: Regression predicts a numeric outcome. Classification predicts an outcome that it is an ordered or unordered set of qualitative values.

The modeling workflow usually starts with Exploratory Data Analysis (EDA), then continues with initial feature engineering, initial models and model tuning, model evaluation, more feature engineering, refined models and model tuning, and lastly, final model evaluation. 

EDA consists of numerical analysis and visualization where different discoveries lead to more questions and "side-quests" to gain more understanding.

Feature engineering contains specific model terms that make easier to accurately model the observed data. This can include complex methodologies such as PCA, or simple features (using the ratio of two predictors).

Model tuning and selection: some models require parameter tuning where some structural parameters are required to be specific or optimized.

Model evaluation: Assess the model's performance metrics, examine residual plots and conduct other EDA-like analyses to understand how well the models work.

Refer to the table of cases used in <https://www.tmwr.org/software-modeling.html#predictive-models>

Chapters 2 and 3 not covered here.

## 4. Ames housing data


```{r}
library(modeldata)
library(ggplot2)
library(tidyverse)
data(ames)
dim(ames)

# Exploring variables
ggplot(ames,aes(Sale_Price)) + geom_histogram(bins=50)

ggplot(ames,aes(Sale_Price)) + geom_histogram(bins=50) + scale_x_log10()

# Transform sale price into log
ames = ames %>%
  mutate(Sale_Price = log10(Sale_Price))

```

## 5. Spending our data

The idea of data spending is an important first consideration hwen modeling, especially as it relates to empirical validation. When there are copious amounts of data available, a smart strategy is to allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only. 

### 5.1 Common methods for splitting data

The primary approach for empirical model validation is to split the existing pool of data into two distinct sets. Some observations are used to develop and optimize the model. This training set is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated. The other portion of the observations are placed into the test set. This is held in reserve until one or two models are chosen as the methods that are mostly likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. It is critical to only look at the test set once, otherwise it becomes part of the modeling process.

```{r}
library(rsample)

# set random seed
set.seed(123)

# split train / test
ames_split = initial_split(ames,prob=0.80)
ames_split

```


```{r}
ames_train = training(ames_split)
ames_test = testing(ames_split)
dim(ames_train)
```
When splitting randomly, we might find class imbalance, one class occurs much less frequently than another. To avoid this, stratified sampling can be used. The train/test is conducted separately within each class and then subsamples are combined into the overall training and test set. For regression problems, the outcome data can be artificially binned into quartiles and then stratified sampling conducted four separate times.This is an effective method for keeping the distributions of the outcome similar between training and test set.


```{r}
set.seed(123)
ames_split = initial_split(ames,prob=0.8,strata=Sale_Price)
ames_train = training(ames_split)
ames_test = testing(ames_split)
dim(ames_train)

```


### 5.2 What proportion should be used?

The amount of data that should be allocated when splitting the data is highly dependent on the context of the problem at hand. Too much data in the training set lowers the quality of the performance estimates. Conversely, too much data in the test set handicaps the model's ability to find appropriate parameter estimates.Test set should be avoided only when the data are pathologically small. 

### 5.3 What about a validation set?

How can we tell what is best if we don't measure performance until the test set?. It is common to hear about validation sets as an answer to this question, especially in the neural network and deep learning literature. The validation set was originally defined in the early days of neural networks when researchers realized that measuring performance by re-predicting the training set samples led to results that were overly optimistic. This led to models that overfit, models that performed very well on the training set but poorly in the test set. To combat this issue, a small validation set of data were held back and used to measure performance as the network was trained. Once the validation set error rate began to rise, the training would be halted. The validation set was a means to get a rough sense of how well the model performed prior to the test set.

### 5.4 Multi-level data

With the Ames housing data, a property is considered to be the independent experimental unit. It is safe to assume  that, statistically, the data from a property are independent. For other applications, that is not always the case.

- For longitudinal data, the same independent experimental unit can be measured over multiple time points. An example would be a human subject in a medical trial.

- A batch of manufactured product might also be considered the independent experimental unit. In repeated measures designs, replicate data points from a batch are collected.

In these situations, the data set will have multiple rows per experimental unit. Simple resampling across rows would lead to some data within an experimental unit being in the training set and others in the test set. Data splitting should occur at the independent experimental unit level of the data. For example, to produce an 80/20 split of the data, 80% of the experimental units should be allocated for the training set.

## 6. Feature engineering with recipes

Feature engineering encompasses activities that reformat predictor values to make them easier for a model to use effectively. This includes transformations and encodings of the data to best represent their important characteristics. Imagine that you have two predictors in a data set that can be more effectively represented in your model of interest as a ratio; creating a new predictor from the ratio of the original two is a simple example of feature engineering.

Take the location of a house in Ames as a more involved example. There are a variety of ways that this spatial information can be exposed to a model, including neighborhood (a qualitative measure), longitude/latitude, distance to the nearest school or Iowa State University, and so on.

Feature engineering and data preprocessing can also involve reformatting required by the model. Some models use geometric distance metrics and, consequently, numeric predictors should be centered and scaled so that they are all in the same units.

### 6.1 A simple recipe for the Ames housing data


In this section, we will focus on a small subset of the predictors available in the Ames housing data:

- The neighborhood (qualitative, with 29 neighborhoods in the training set)

- The general living area (continuous, named Gr_Liv_Area)

- The year built (Year_Built)

- The type of building (Bldg_Type with values OneFam (n=1,814), TwoFmCon (n=45), Duplex (n=76), Twnhs (n=76), and TwnhsE (n=188))



```{r}
library(tidymodels) # Includes the recipes package

simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
         data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_dummy(all_nominal())
simple_ames
```


### 6.2 Using recipes

When invoking the recipe() function, the steps are not estimated or executed in any way. The second phase for using a recipe is to estimate any quantities required by the steps using the prep() function. For example, we can use step_normalize() to center and scale any predictors selected in the step. When we call prep(recipe, training), this function estimates the required means and standard deviations from the data in the training argument. The transformations specified by each step are also sequentially executed on the data set. Again using normalization as the example, the means and variances are estimated and then used to standardize the columns.


```{r}
simple_ames <- prep(simple_ames, training = ames_train)
simple_ames
```

The third phase of recipe usage is to apply the preprocessing operations to a data set using the bake() function. The bake() function can apply the recipe to any data set. To use the test set, the syntax would be:


```{r}
test_ex <- bake(simple_ames, new_data = ames_test)
names(test_ex) %>% head()
```

The bake() function can also take selectors so that, if we only wanted the neighborhood results, we could use:

```{r}
bake(simple_ames, ames_test, starts_with("Neighborhood_"))
```

Using a recipe there is a three phase process:
recipe(): defines the pre-processing
prep(): calculates statistics form the training set
bake(): applies the preprocessing to data sets


### 6.3 Encoding qualitative data in a numeric format

One of the most common feature engineering tasks is transforming nominal or qualitative data (factors or characters) so that they can be encoded or represented numerically. Sometimes we can alter the factor levels of a qualitative column in helpful ways prior to such a transformation. For example, step_unknown() can be used to change missing values to a dedicated factor level. Similarly, if we anticipate that a new factor level may be encountered in future data, step_novel() can allot a new level for this purpose.

Additionally, step_other() can be used to analyze the frequencies of the factor levels in the training set and convert infrequently occurring values to a catch-all level of “other”, with a specific threshold that can be specified.

```{r}
ggplot(ames_train, aes(y = Neighborhood)) + 
  geom_bar() + 
  labs(y = NULL)
```

Here there are two neighborhoods that have less than five properties in the training data; in this case, no houses at all in the Landmark neighborhood were included in the training set. For some models, it may be problematic to have dummy variables with a single non-zero entry in the column. At a minimum, it is highly improbable that these features would be important to a model. If we add step_other(Neighborhood, threshold = 0.01) to our recipe, the bottom 1% of the neighborhoods will be lumped into a new level called “other”. In this training set, this will catch 9 neighborhoods.


```{r}
simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
         data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal())
```


There are a few strategies for converting a factor predictor to a numeric format. The most common method is to create “dummy” or indicator variables.

Another method that is useful when there are a large number of categories is called effect or likelihood encodings. This method replaces the original data with a single numeric column that measures the effect of those data. For example, for the neighborhood predictor, the mean sale price is computed for each neighborhood and these means are substituted for the original data values. This can be effective but should be used with care. 


### 6.4 Interaction terms

Interaction effects involve two or more predictors. Such an effect occurs when one predictor has an effect on the outcome that is contingent on one or more other predictors.
For example, if you were trying to predict your morning commute time, two potential predictors could be the amount of traffic and the time of day. However, the relationship between commute time and the amount of traffic is different for different times of day. In this case, you could add an interaction term between the two predictors to the model along with the original two predictors (which are called the “main effects”). Numerically, an interaction term between predictors is encoded as their product. 

Interactions are only defined in terms of their effect on the outcome and can be combinations of different types of data (e.g., numeric, categorical, etc).

After exploring the Ames training set, we might find that the regression slopes for the general living area differ for different building types:

```{r}
ggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) + 
  geom_point(alpha = .2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = "red") + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "General Living Area", y = "Sale Price (USD)")
```

How are interactions specified in a recipe?

```{r}
Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Bldg_Type + log10(Gr_Liv_Area):Bldg_Type
# or
Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type 
```

where * expands those columns to the main effects and interaction term. Again, the formula method does many things simultaneously and understands that a factor variable (such as Bldg_Type) should be expanded into dummy variables first and that the interaction should involve all of the resulting binary columns

Recipes are more explicit and sequential, and give you more control. With the current recipe, step_dummy() has already created dummy variables. How would we combine these for an interaction? The additional step would look like step_interact(~ interaction terms) where the terms on the right-hand side of the tilde are the interactions. These can include selectors, so it would be appropriate to use:

```{r}
simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
         data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal()) %>% 
  # Gr_Liv_Area is on the log scale from a previous step
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") )
```


### 6.6 Other examples of recipe steps

#### Spline functions

When a predictor has a nonlinear relationship with the outcome, some types of predictive models can adaptively approximate this relationship during training. However, simpler is usually better and it is not uncommon to try to use a simple model, such as a linear fit, and add in specific non-linear features for predictors that may need them. One common method for doing this is to use spline functions to represent the data. Splines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, non-linear relationship.


```{r}
library(patchwork)
library(splines)

plot_smoother <- function(deg_free) {
  ggplot(ames_train, aes(x = Latitude, y = Sale_Price)) + 
    geom_point(alpha = .2) + 
    scale_y_log10() +
    geom_smooth(
      method = lm,
      formula = y ~ ns(x, df = deg_free),
      col = "red",
      se = FALSE
    ) +
    ggtitle(paste(deg_free, "Spline Terms"))
}

( plot_smoother(2) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) )
```

Some panels clearly fit poorly; two terms under-fit the data while 100 terms over-fit. The panels with five and 20 terms seem like reasonably smooth fits that catch the main patterns of the data. This indicates that the proper amount of “non-linear-ness” matters. The number of spline terms could then be considered a tuning parameter for this model.

#### Feature extraction

Another common method for representing multiple features at once is called feature extraction. Most of these techniques create new features from the predictors that capture the information in the broader set as a whole. For example, principal component analysis (PCA) tries to extract as much of the original information in the predictor set as possible using a smaller number of features. PCA is a linear extraction method, meaning that each new feature is a linear combination of the original predictors. One nice aspect of PCA is that each of the new features, called the principal components or PCA scores, are uncorrelated with one another.

In the Ames data, there are several predictors that measure size of the property, such as the total basement size (Total_Bsmt_SF), size of the first floor (First_Flr_SF), the general living area (Gr_Liv_Area), and so on. PCA might be an option to represent these potentially redundant variables as a smaller feature set. Apart from the general living area, these predictors have the suffix SF in their names (for square feet) so a recipe step for PCA might look like:


```{r}
# Use a regular expression to capture house size predictors: 
  #step_pca(matches("(SF$)|(Gr_Liv)"))
```

Note that all of these columns are measured in square feet. PCA assumes that all of the predictors are on the same scale. That’s true in this case, but often this step can be preceded by step_normalize(), which will center and scale each column.

#### Row sampling steps

Recipe steps can affect the rows of a data set as well. For example, the previously mentioned subsampling technique for class imbalances will change the data being given to the model. There are several possible approaches to try:

- Downsampling the data keeps the minority class and takes a random sample of the majority class so that class frequencies are balanced.

- Upsampling replicates samples from the minority class to balance the classes. Some techniques do this by synthesizing new samples that resemble the minority class data while other methods simply add the same minority samples repeatedly.

themis package has recipe steps that can be used for this purpose. For simple down-sampling, we would use:

```{r}
#step_downsample(outcome_column_name)
```


There are other step functions that are row-based as well: step_filter(), step_sample(), step_slice(), and step_arrange(). In almost all uses of these steps, the skip argument should be set to TRUE.



















































