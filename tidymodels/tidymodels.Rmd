---
title: "Tidymodels"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

Tidymodels: <https://www.tmwr.org/>.

Models are mathematical tools that can describe a system and capture relationships in the data given to them. The utility of a model hinges on its ability to be reductive.

### 1.1. Types of models

#### Descriptive models

The purpose of a descriptive model is to describe or illustrate characteristics of some data. The analysis might have no other purpose than to visually emphasize some trend or artifact in the data. An example can be a LOESS (locally estimated smoothing model) model, that consists on a smooth and flexible regression model. This model is used to discover potential ways to represent a variable.

#### Inferential models

The goal of an inferential model is to produce a decision for a research question or to test a specific hypothesis. It aims to make some statement of truth regarding a predefined conjecture or idea. In many cases, a qualitative statement is produced (e.g, a difference was "statistically significant").

For example, in a clinical trial, the goal might be to provide confirmation that a new therapy does a better job in prolonging life than an alternative, like an existing therapy or no treatment. If the clinical endpoint was related to survival of a patient, the null hypothesis might be that the two therapeutic groups have equal median survival times with the alternative hypothesis being that the new therapy has higher median survival. If this trial were evaluated using the traditional null hypothesis significance testing, a p-value would be produced using some predefined methodology. Small values of the p-value indicate that there is evidence that the new therapy does help patients live longer. If not, the trial would conclude that there is a failure to show such a difference, which could be due to a number of reasons.

Inferential techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. There tends to be a delayed feedback loop in understanding how well the data matches the assumptions. In the clinical trial example, if statistical significance indicate that the new therapy should be available for patients to use, it still may be years before it is used in the field and enough data are generated for an independent assessment of whether the original statistical analysis led to the appropriate decision.

#### Predictive models

They model data to produce the most accurate prediction possible for new data. The primary goal is that the predicted values have the highest possible fidelity to the true value of the new data. A simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month. For this type of model, the problem type is one of estimation rather than inference. 

### 1.2 Terminology

Many models can be categorised as being supervised or unsupervised. Unsupervised models are those that learn patterns, clusters or other characteristics of the data but lack an outcome variable. PCA, cluster or autoencoders are examples of unsupervised models.They are used to understand relationships between variables or sets of variables without an explicit relationship between variables and an outcome. Supervised models are those that have an outcome variable. Within supervised models, there are two main sub-categories: Regression predicts a numeric outcome. Classification predicts an outcome that it is an ordered or unordered set of qualitative values.

The modeling workflow usually starts with Exploratory Data Analysis (EDA), then continues with initial feature engineering, initial models and model tuning, model evaluation, more feature engineering, refined models and model tuning, and lastly, final model evaluation. 

EDA consists of numerical analysis and visualization where different discoveries lead to more questions and "side-quests" to gain more understanding.

Feature engineering contains specific model terms that make easier to accurately model the observed data. This can include complex methodologies such as PCA, or simple features (using the ratio of two predictors).

Model tuning and selection: some models require parameter tuning where some structural parameters are required to be specific or optimized.

Model evaluation: Assess the model's performance metrics, examine residual plots and conduct other EDA-like analyses to understand how well the models work.

Refer to the table of cases used in <https://www.tmwr.org/software-modeling.html#predictive-models>

Chapters 2 and 3 not covered here.

## 4. Ames housing data


```{r}
library(modeldata)
library(ggplot2)
library(tidyverse)
data(ames)
dim(ames)

# Exploring variables
ggplot(ames,aes(Sale_Price)) + geom_histogram(bins=50)

ggplot(ames,aes(Sale_Price)) + geom_histogram(bins=50) + scale_x_log10()

# Transform sale price into log
ames = ames %>%
  mutate(Sale_Price = log10(Sale_Price))

```

## 5. Spending our data

The idea of data spending is an important first consideration hwen modeling, especially as it relates to empirical validation. When there are copious amounts of data available, a smart strategy is to allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only. 

### 5.1 Common methods for splitting data

The primary approach for empirical model validation is to split the existing pool of data into two distinct sets. Some observations are used to develop and optimize the model. This training set is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated. The other portion of the observations are placed into the test set. This is held in reserve until one or two models are chosen as the methods that are mostly likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. It is critical to only look at the test set once, otherwise it becomes part of the modeling process.

```{r}
library(rsample)

# set random seed
set.seed(123)

# split train / test
ames_split = initial_split(ames,prob=0.80)
ames_split

```


```{r}
ames_train = training(ames_split)
ames_test = testing(ames_split)
dim(ames_train)
```
When splitting randomly, we might find class imbalance, one class occurs much less frequently than another. To avoid this, stratified sampling can be used. The train/test is conducted separately within each class and then subsamples are combined into the overall training and test set. For regression problems, the outcome data can be artificially binned into quartiles and then stratified sampling conducted four separate times.This is an effective method for keeping the distributions of the outcome similar between training and test set.


```{r}
set.seed(123)
ames_split = initial_split(ames,prob=0.8,strata=Sale_Price)
ames_train = training(ames_split)
ames_test = testing(ames_split)
dim(ames_train)

```


### 5.2 What proportion should be used?

The amount of data that should be allocated when splitting the data is highly dependent on the context of the problem at hand. Too much data in the training set lowers the quality of the performance estimates. Conversely, too much data in the test set handicaps the model's ability to find appropriate parameter estimates.Test set should be avoided only when the data are pathologically small. 

### 5.3 What about a validation set?

How can we tell what is best if we don't measure performance until the test set?. It is common to hear about validation sets as an answer to this question, especially in the neural network and deep learning literature. The validation set was originally defined in the early days of neural networks when researchers realized that measuring performance by re-predicting the training set samples led to results that were overly optimistic. This led to models that overfit, models that performed very well on the training set but poorly in the test set. To combat this issue, a small validation set of data were held back and used to measure performance as the network was trained. Once the validation set error rate began to rise, the training would be halted. The validation set was a means to get a rough sense of how well the model performed prior to the test set.

### 5.4 Multi-level data

With the Ames housing data, a property is considered to be the independent experimental unit. It is safe to assume  that, statistically, the data from a property are independent. For other applications, that is not always the case.

- For longitudinal data, the same independent experimental unit can be measured over multiple time points. An example would be a human subject in a medical trial.

- A batch of manufactured product might also be considered the independent experimental unit. In repeated measures designs, replicate data points from a batch are collected.

In these situations, the data set will have multiple rows per experimental unit. Simple resampling across rows would lead to some data within an experimental unit being in the training set and others in the test set. Data splitting should occur at the independent experimental unit level of the data. For example, to produce an 80/20 split of the data, 80% of the experimental units should be allocated for the training set.






















